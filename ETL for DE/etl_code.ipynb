{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da2ba140-6a95-49fe-8e89-77686a9364bf",
   "metadata": {},
   "source": [
    "\n",
    "EXTRACTION "
   ]
  },
  {
   "cell_type": "raw",
   "id": "b619d344-d5e3-4117-80e8-883cb520fe5f",
   "metadata": {},
   "source": [
    "I have developed the functions to extract the data from different file formats. As there is different functions for the file formats, i have written one function each for the .csv, .json, and the .xml filetypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fb7455f-df61-4e30-9e1b-3da8a651060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob # for path of file\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14b88a41-4454-4252-b43d-4217f933e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"log_file\"\n",
    "target_file = \"transformed_data.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34a026cb-a7b0-4638-97a4-6509cb1baf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract from CSV file \n",
    "def extract_from_csv(file_to_process):\n",
    "    df = pd.read_csv(file_to_process)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aabe8468-01b7-4336-930b-c6ea2cec02f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract from JSON file \n",
    "def extract_from_json(file_to_process):\n",
    "    df = pd.read_json(file_to_process, lines=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a3aa2e2-10da-4ee7-8376-b7183d97880d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "0  1  3\n",
      "1  2  4\n",
      "2  5  6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrames\n",
    "df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "df2 = pd.DataFrame({'A': [5], 'B': [6]})\n",
    "\n",
    "# Use concat to add data\n",
    "result = pd.concat([df1, df2], ignore_index=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81d49c0a-660f-4f1a-8a07-c294253a30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract from XML file \n",
    "\n",
    "def extract_from_xml(file_to_process): \n",
    "    dataframe = pd.DataFrame(columns=[\"name\", \"height\", \"weight\"]) \n",
    "    tree = ET.parse(file_to_process) \n",
    "    root = tree.getroot() \n",
    "    for person in root: \n",
    "        name = person.find(\"name\").text \n",
    "        height = float(person.find(\"height\").text) \n",
    "        weight = float(person.find(\"weight\").text) \n",
    "        dataframe = pd.concat([dataframe, pd.DataFrame([{\"name\":name, \"height\":height, \"weight\":weight}])], ignore_index=True) \n",
    "    return dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0d880cc-026b-482e-b433-f06982bd19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(): \n",
    "    extracted_data = pd.DataFrame(columns=['name','height','weight']) # create an empty data frame to hold extracted data \n",
    "     \n",
    "    # process all csv files, except the target file\n",
    "    for csvfile in glob.glob(\"*.csv\"): \n",
    "        if csvfile != target_file:  # check if the file is not the target file\n",
    "            extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_csv(csvfile))], ignore_index=True) \n",
    "         \n",
    "    # process all json files \n",
    "    for jsonfile in glob.glob(\"*.json\"): \n",
    "        extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_json(jsonfile))], ignore_index=True) \n",
    "     \n",
    "    # process all xml files \n",
    "    for xmlfile in glob.glob(\"*.xml\"): \n",
    "        extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_xml(xmlfile))], ignore_index=True) \n",
    "         \n",
    "    return extracted_data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1b7224-a10f-4974-9bda-3dfc748de35b",
   "metadata": {},
   "source": [
    "TRANSFORMATION "
   ]
  },
  {
   "cell_type": "raw",
   "id": "0da7a84b-6642-4dea-a761-f0132eeabbc4",
   "metadata": {},
   "source": [
    "The height in the extracted data is in inches, and the weight is in pounds. However, the height is required to be in meters, and the weight is required to be in kilograms, rounded to two decimal places. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d182b94-5a72-4c7a-b82a-e5399f8345c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data): \n",
    "    data['height'] = round(data.height * 0.0254, 2)\n",
    "    data['weight'] = round(data.weight * 0.45359237, 2)\n",
    "\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f11293f-d8ae-47a1-b00c-1a828c0e1b2b",
   "metadata": {},
   "source": [
    "LOADING AND LOGGING"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3618ab3-8d3c-4f5e-aaf0-8c9ca3ba4aec",
   "metadata": {},
   "source": [
    "loading the transformed data to a CSV file that  can used to load to a database as per requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ded5641d-6785-4da4-9a97-7af35f6b6de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(target_file, transformed_data): \n",
    "    transformed_data.to_csv(target_file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "794f0e7a-8188-4849-9d2c-819a2dfec76c",
   "metadata": {},
   "source": [
    "implementing the logging operation to record the progress of the different operations. For this operation, recording a message, along with its timestamp, in the log_file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8dfb717-3049-47f9-a8e0-f1206323bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(message): \n",
    "    timestamp_format = '%Y-%h-%d-%H:%M:%S'\n",
    "    now = datetime.now() # get current timestamp \n",
    "    timestamp = now.strftime(timestamp_format) \n",
    "    with open(log_file,\"a\") as f: \n",
    "        f.write(timestamp + ',' + message + '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0b80d0-6ea7-4b25-953b-770db10f80b0",
   "metadata": {},
   "source": [
    "TESTING AND LOGGING "
   ]
  },
  {
   "cell_type": "raw",
   "id": "24fa4790-dda8-4171-99fc-c771a1c091e1",
   "metadata": {},
   "source": [
    "logging refers to recording important events or progress messages during the execution of the ETL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bf50d1b-63f7-4c30-8e2e-3d147b0c1366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the initialization of the ETL process \n",
    "log_progress(\"ETL Job Started\") \n",
    " \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23cfddf8-2cfa-4474-a145-307934d6d210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SANGAM ACHARYA\\AppData\\Local\\Temp\\ipykernel_21900\\114048372.py:7: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_csv(csvfile))], ignore_index=True)\n",
      "C:\\Users\\SANGAM ACHARYA\\AppData\\Local\\Temp\\ipykernel_21900\\3993395751.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, pd.DataFrame([{\"name\":name, \"height\":height, \"weight\":weight}])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Log the beginning of the Extraction process \n",
    "log_progress(\"Extract phase Started\") \n",
    "extracted_data = extract() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec0bd821-09ad-4896-8208-8aa521f327be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the completion of the Extraction process \n",
    "log_progress(\"Extract phase Ended\") \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f382a9da-28dd-4f13-9eae-a81e70fa8ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Data\n",
      "     name  height  weight\n",
      "0    alex    1.67   51.25\n",
      "1    ajay    1.82   61.91\n",
      "2   alice    1.76   69.41\n",
      "3    ravi    1.73   64.56\n",
      "4     joe    1.72   65.45\n",
      "5    jack    1.74   55.93\n",
      "6     tom    1.77   64.18\n",
      "7   tracy    1.78   61.90\n",
      "8    john    1.72   50.97\n",
      "9   simon    1.72   50.97\n",
      "10  jacob    1.70   54.73\n",
      "11  cindy    1.69   57.81\n",
      "12   ivan    1.72   51.77\n"
     ]
    }
   ],
   "source": [
    "# Log the beginning of the Transformation process \n",
    "log_progress(\"Transform phase Started\") \n",
    "transformed_data = transform(extracted_data) \n",
    "print(\"Transformed Data\") \n",
    "print(transformed_data) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f571aa8c-2436-4014-9140-48ecf5b9f082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the completion of the Transformation process \n",
    "log_progress(\"Transform phase Ended\") \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "011e9a9d-15f1-4951-853c-02460f1530fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the beginning of the Loading process \n",
    "log_progress(\"Load phase Started\") \n",
    "load_data(target_file,transformed_data) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c692d33-8ba4-490e-8eb3-ae99f86aafba",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Log the completion of the Loading process \n",
    "log_progress(\"Load phase Ended\") \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8431382e-d44b-4a36-bcd0-8bd3fb96c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the completion of the ETL process \n",
    "log_progress(\"ETL Job Ended\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f078bb6-30f9-405a-96fa-52755a93c4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
